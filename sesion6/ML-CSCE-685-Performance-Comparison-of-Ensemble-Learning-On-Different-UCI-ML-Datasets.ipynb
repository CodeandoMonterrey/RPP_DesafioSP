{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal behind **ensemble methods** is to combine different classifiers into a meta-classifier that has a better generalization performance than each individual classifier alone.[2]\n",
    "\n",
    "To illustrate why ensemble methods can work better than individual classifiers alone, let's apply the simple concepts of combinatorics. For the following example, we make the assumption that all $n$ base classifiers for a binary classification task have an equal error rate \u001f$\\varepsilon$. Furthermore, we assume that the classifiers are independent and the error rates are not correlated. Under those assumptions, we can simply express the error probability of an ensemble of base classifiers as a probability mass function of a binomial distribution:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$P\\left(y\\geq k\\right)=\\varepsilon_{ensemble}={\\displaystyle \\sum_{k}^{n}}\\binom{n}{k}\\varepsilon^{k}\\left(1-\\varepsilon\\right)^{n-k}$\n",
    "\n",
    "Here, $\\binom{n}{k}$ is the binomial coefficient $n$ $choose$ $k$. In other words, we compute the probability that the prediction of the ensemble is wrong. Now let's take a look at a more concrete example of 25 base classifiers ($n=25$) with an error rate of\n",
    "0.35 $(\\varepsilon=0.35\u001f)$: [1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\varepsilon_{ensemble}={\\displaystyle \\sum_{k=13}^{25}}\\binom{25}{k}0.35^{k}\\left(1-0.35\\right)^{25-k}=0.06$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Types of Ensemble Methods**  \n",
    "**Bagging** - It is also known as bootstrap aggregating, is a technique that repeatedly samples (with replacement) from a data set according to a uniform probability distribution. Each bootstrap sample has the same size as the original data. Because the sampling is done with replacement, some instances may appear several times in the same training set, while others may be omitted from the training set. On average, a bootstrap sample $D_i$ contains approximately 63% of the original training data.  \n",
    "**Boosting** - It is an iterative procedure used to adaptively change the distribution of training examples so that the base classifiers will focus on examples that are hard to classify. Unlike bagging, boosting assigns weight to each training example and may adaptively change the weight at the end of each boosting round.  \n",
    "**Random Forest** - This is a class of ensemble methods specifically designed for decision tree classifiers. It combines the prediction made by multiple decision tree, where each tree is generated based on the values of an independent set of random vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table border=\"1\" align='left' cellpadding=\"0\" cellspacing=\"0\">\n",
    "    <tr>\n",
    "        <th>Data Set</th>\n",
    "        <th>Number of (Records, Attributes, Classes)</th>\n",
    "        <th>Accuracy Type</th>\n",
    "        <th>Decision Tree Accuracy(%)</th>\n",
    "        <th>Bagging Accuracy(%)</th>\n",
    "        <th>Boosting Accuracy(%)</th>\n",
    "        <th>Random Forest Accuracy(%)</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td rowspan=\"2\">Wine</td>\n",
    "        <td rowspan=\"2\">(178, 14, 3)</td>\n",
    "        <td>train</td>\n",
    "        <td>100</td>\n",
    "        <td>99.9</td>\n",
    "        <td>100</td>\n",
    "        <td>100</td>\n",
    "    </tr>\n",
    "    <tr>        \n",
    "        <td>test</td>\n",
    "        <td>90.4</td>\n",
    "        <td>95.0</td>\n",
    "        <td>90.9</td>\n",
    "        <td>98.9</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td rowspan=\"2\">Pima Indians Diabetes</td>\n",
    "        <td rowspan=\"2\">(768, 8, 2)</td>\n",
    "        <td>train</td>\n",
    "        <td>100</td>\n",
    "        <td>99.9</td>\n",
    "        <td>100</td>\n",
    "        <td>100</td>\n",
    "    </tr>\n",
    "    <tr>        \n",
    "        <td>test</td>\n",
    "        <td>68.6</td>\n",
    "        <td>74.7</td>\n",
    "        <td>68.6</td>\n",
    "        <td>76.3</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td rowspan=\"2\">Breast Cancer</td>\n",
    "        <td rowspan=\"2\">(569, 30, 2)</td>\n",
    "        <td>train</td>\n",
    "        <td>100.0</td>\n",
    "        <td>100.0</td>\n",
    "        <td>100.0</td>\n",
    "        <td>100.0</td>\n",
    "    </tr>\n",
    "    <tr>        \n",
    "        <td>test</td>\n",
    "        <td>93.0</td>\n",
    "        <td>96.3</td>\n",
    "        <td>93.0</td>\n",
    "        <td>96.7</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td rowspan=\"2\">German Credit</td>\n",
    "        <td rowspan=\"2\">(1000, 24, 2)</td>\n",
    "        <td>train</td>\n",
    "        <td>100.0</td>\n",
    "        <td>100.0</td>\n",
    "        <td>100.0</td>\n",
    "        <td>100.0</td>\n",
    "    </tr>\n",
    "    <tr>        \n",
    "        <td>test</td>\n",
    "        <td>69.4</td>\n",
    "        <td>77.3</td>\n",
    "        <td>68.5</td>\n",
    "        <td>75.9</td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.misc import comb\n",
    "import math\n",
    "\n",
    "def ensemble_error(n_classifier, error):\n",
    "    k_start = math.ceil(n_classifier / 2.0)\n",
    "    probs = [comb(n_classifier, k) * error**k * (1-error)**(n_classifier-k)\n",
    "                for k in range (k_start, n_classifier + 1)]\n",
    "    return sum(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.060444913567020482"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 25 base classifiers and each with base\n",
    "ensemble_error(25, 0.35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "error_range = np.arange(0.0, 1.01, 0.01)\n",
    "ens_errors = [ensemble_error(n_classifier=11, error=error)\n",
    "                  for error in error_range]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEMCAYAAADNtWEcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x8dafda0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style='whitegrid', context='notebook')\n",
    "\n",
    "%matplotlib inline\n",
    "plt.plot(error_range, ens_errors, label='Ensemble error', linewidth=2)\n",
    "plt.plot(error_range, error_range, linestyle='--', label='Base error', linewidth=2)\n",
    "plt.xlabel('Base error')\n",
    "plt.ylabel('Ensemble error')\n",
    "plt.xlim(-0.05, 1.05)\n",
    "plt.ylim(-0.05, 1.05)\n",
    "plt.legend(loc='best')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def benchmark_clf(clf_name, clf, X, y, kfold):\n",
    "    train_scores, test_scores = [], []\n",
    "    for k, (train, test) in enumerate(kfold):\n",
    "        X_train = X.iloc[train]\n",
    "        y_train = y[train]\n",
    "        X_test = X.iloc[test]\n",
    "        y_test = y[test]\n",
    "        \n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        y_train_pred = clf.predict(X_train)\n",
    "        y_test_pred = clf.predict(X_test)\n",
    "\n",
    "        train_score = metrics.accuracy_score(y_train, y_train_pred)\n",
    "        test_score = metrics.accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "        #score = pipe_lr.score(X_train[test], y_train[test])\n",
    "        train_scores.append(train_score)\n",
    "        test_scores.append(test_score)\n",
    "        #print('Fold: %s, Class dist.: %s, Acc: %.3f' % (k+1, np.bincount(y_train[train]), score))\n",
    "\n",
    "    #print('CV accuracy scores: %s' % scores)    \n",
    "    print('%s train/test accuracies %.3f/%.3f' % (clf_name, np.mean(train_scores), np.mean(test_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.cross_validation import train_test_split, StratifiedKFold\n",
    "\n",
    "def split_data_and_benchmark_clf(X, y):\n",
    "    kfold = StratifiedKFold(y=y, n_folds=10, random_state=1)\n",
    "\n",
    "    tree = DecisionTreeClassifier(criterion='entropy', \n",
    "                                  max_depth=None,\n",
    "                                  random_state=1)\n",
    "\n",
    "    # BaggingClassifier with BaseEstimators of 50 DecisionTreeClassifiers\n",
    "    bag = BaggingClassifier(base_estimator=tree,\n",
    "                            n_estimators=50, \n",
    "                            bootstrap=True, \n",
    "                            bootstrap_features=False, \n",
    "                            n_jobs=1, \n",
    "                            random_state=1)\n",
    "\n",
    "    # AdaBoostClassifier with BaseEstimators of 50 DecisionTreeClassifiers\n",
    "    ada = AdaBoostClassifier(base_estimator=tree,\n",
    "                             n_estimators=50, \n",
    "                             learning_rate=0.1,\n",
    "                             random_state=1)\n",
    "    \n",
    "    # RandomForestClassifier with BaseEstimators of 50 DecisionTreeClassifiers\n",
    "    rnf = RandomForestClassifier(n_estimators=50,\n",
    "                             random_state=0)\n",
    "    \n",
    "    # Benchmark Decision Tree\n",
    "    benchmark_clf('Decision tree', tree, X, y, kfold)\n",
    "\n",
    "    # Benchmark BaggingClassifier with BaseEstimator as a DecisionTreeClassifier\n",
    "    benchmark_clf('Bagging', bag, X, y, kfold)\n",
    "    \n",
    "    # Benchmark AdaBoostClassifier with BaseEstimator as a DecisionTreeClassifier\n",
    "    benchmark_clf('AdaBoost', ada, X, y, kfold)\n",
    "    \n",
    "    # Benchmark RandomForestClassifier with BaseEstimator as a DecisionTreeClassifier\n",
    "    benchmark_clf('RandomForest', rnf, X, y, kfold)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wine Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data'\n",
    "col_names = ['Class', 'Alcohol', 'Malic acid', 'Ash', \n",
    "'Alcalinity of ash', 'Magnesium', 'Total phenols', \n",
    "'Flavanoids', 'Nonflavanoid phenols', 'Proanthocyanins', \n",
    "'Color intensity', 'Hue', 'OD280/OD315 of diluted wines', 'Proline']\n",
    "wine_df = pd.read_csv(url, header=None, names = col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "      <th>Alcohol</th>\n",
       "      <th>Malic acid</th>\n",
       "      <th>Ash</th>\n",
       "      <th>Alcalinity of ash</th>\n",
       "      <th>Magnesium</th>\n",
       "      <th>Total phenols</th>\n",
       "      <th>Flavanoids</th>\n",
       "      <th>Nonflavanoid phenols</th>\n",
       "      <th>Proanthocyanins</th>\n",
       "      <th>Color intensity</th>\n",
       "      <th>Hue</th>\n",
       "      <th>OD280/OD315 of diluted wines</th>\n",
       "      <th>Proline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Class  Alcohol  Malic acid   Ash  Alcalinity of ash  Magnesium  \\\n",
       "0      1    14.23        1.71  2.43               15.6        127   \n",
       "1      1    13.20        1.78  2.14               11.2        100   \n",
       "2      1    13.16        2.36  2.67               18.6        101   \n",
       "3      1    14.37        1.95  2.50               16.8        113   \n",
       "4      1    13.24        2.59  2.87               21.0        118   \n",
       "\n",
       "   Total phenols  Flavanoids  Nonflavanoid phenols  Proanthocyanins  \\\n",
       "0           2.80        3.06                  0.28             2.29   \n",
       "1           2.65        2.76                  0.26             1.28   \n",
       "2           2.80        3.24                  0.30             2.81   \n",
       "3           3.85        3.49                  0.24             2.18   \n",
       "4           2.80        2.69                  0.39             1.82   \n",
       "\n",
       "   Color intensity   Hue  OD280/OD315 of diluted wines  Proline  \n",
       "0             5.64  1.04                          3.92     1065  \n",
       "1             4.38  1.05                          3.40     1050  \n",
       "2             5.68  1.03                          3.17     1185  \n",
       "3             7.80  0.86                          3.45     1480  \n",
       "4             4.32  1.04                          2.93      735  "
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(178, 13)\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "X = wine_df.drop('Class', axis = 1)\n",
    "y = wine_df['Class']\n",
    "print(X.shape)\n",
    "print(len(y.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision tree train/test accuracies 1.000/0.904\n",
      "Bagging train/test accuracies 0.999/0.950\n",
      "AdaBoost train/test accuracies 1.000/0.909\n",
      "RandomForest train/test accuracies 1.000/0.989\n"
     ]
    }
   ],
   "source": [
    "split_data_and_benchmark_clf(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pima Indians Diabetes Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pregnant</th>\n",
       "      <th>glucose</th>\n",
       "      <th>bp</th>\n",
       "      <th>skin_fold</th>\n",
       "      <th>serum_insulin</th>\n",
       "      <th>bmi</th>\n",
       "      <th>pedigree</th>\n",
       "      <th>age</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pregnant  glucose  bp  skin_fold  serum_insulin   bmi  pedigree  age  label\n",
       "0         6      148  72         35              0  33.6     0.627   50      1\n",
       "1         1       85  66         29              0  26.6     0.351   31      0\n",
       "2         8      183  64          0              0  23.3     0.672   32      1\n",
       "3         1       89  66         23             94  28.1     0.167   21      0\n",
       "4         0      137  40         35            168  43.1     2.288   33      1"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/pima-indians-diabetes/pima-indians-diabetes.data'\n",
    "col_names = ['pregnant', 'glucose', 'bp', 'skin_fold', 'serum_insulin', 'bmi', 'pedigree', 'age', 'label']\n",
    "pima_df = pd.read_csv(url, header=None, names=col_names)\n",
    "pima_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768, 8)\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "X = pima_df.drop('label', axis = 1)\n",
    "y = pima_df['label']\n",
    "print(X.shape)\n",
    "print(len(y.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision tree train/test accuracies 1.000/0.686\n",
      "Bagging train/test accuracies 0.999/0.747\n",
      "AdaBoost train/test accuracies 1.000/0.686\n",
      "RandomForest train/test accuracies 1.000/0.763\n"
     ]
    }
   ],
   "source": [
    "# do stratified K Fold splitting and Benchmark the data\n",
    "split_data_and_benchmark_clf(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Breast Cancer Wisconsin dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842302</td>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842517</td>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84300903</td>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>...</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84348301</td>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>...</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84358402</td>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>...</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0  1      2      3       4       5        6        7       8   \\\n",
       "0    842302  M  17.99  10.38  122.80  1001.0  0.11840  0.27760  0.3001   \n",
       "1    842517  M  20.57  17.77  132.90  1326.0  0.08474  0.07864  0.0869   \n",
       "2  84300903  M  19.69  21.25  130.00  1203.0  0.10960  0.15990  0.1974   \n",
       "3  84348301  M  11.42  20.38   77.58   386.1  0.14250  0.28390  0.2414   \n",
       "4  84358402  M  20.29  14.34  135.10  1297.0  0.10030  0.13280  0.1980   \n",
       "\n",
       "        9    ...        22     23      24      25      26      27      28  \\\n",
       "0  0.14710   ...     25.38  17.33  184.60  2019.0  0.1622  0.6656  0.7119   \n",
       "1  0.07017   ...     24.99  23.41  158.80  1956.0  0.1238  0.1866  0.2416   \n",
       "2  0.12790   ...     23.57  25.53  152.50  1709.0  0.1444  0.4245  0.4504   \n",
       "3  0.10520   ...     14.91  26.50   98.87   567.7  0.2098  0.8663  0.6869   \n",
       "4  0.10430   ...     22.54  16.67  152.20  1575.0  0.1374  0.2050  0.4000   \n",
       "\n",
       "       29      30       31  \n",
       "0  0.2654  0.4601  0.11890  \n",
       "1  0.1860  0.2750  0.08902  \n",
       "2  0.2430  0.3613  0.08758  \n",
       "3  0.2575  0.6638  0.17300  \n",
       "4  0.1625  0.2364  0.07678  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data'\n",
    "#col_names = ['id', 'diagnosis', 'radius', 'texture', 'perimeter', 'area', 'smoothness', 'compactness', 'concavity', 'concave points', 'symmetry', 'fractal dimension']\n",
    "wdbc_df = pd.read_csv(url, header=None)\n",
    "wdbc_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(569, 30)\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "X = wdbc_df.loc[:, 2:]\n",
    "y = wdbc_df.loc[:, 1]\n",
    "print(X.shape)\n",
    "print(len(y.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0], dtype=int64)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "le.transform(['M', 'B'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision tree train/test accuracies 1.000/0.930\n",
      "Bagging train/test accuracies 1.000/0.963\n",
      "AdaBoost train/test accuracies 1.000/0.930\n",
      "RandomForest train/test accuracies 1.000/0.967\n"
     ]
    }
   ],
   "source": [
    "# do stratified K Fold splitting and Benchmark the data\n",
    "split_data_and_benchmark_clf(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### German Credit dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>67</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>48</td>\n",
       "      <td>2</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>49</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>79</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>45</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>3</td>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>53</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0   1   2   3   4   5   6   7   8   9  ...  15  16  17  18  19  20  21  22  \\\n",
       "0   1   6   4  12   5   5   3   4   1  67 ...   0   0   1   0   0   1   0   0   \n",
       "1   2  48   2  60   1   3   2   2   1  22 ...   0   0   1   0   0   1   0   0   \n",
       "2   4  12   4  21   1   4   3   3   1  49 ...   0   0   1   0   0   1   0   1   \n",
       "3   1  42   2  79   1   4   3   4   2  45 ...   0   0   0   0   0   0   0   0   \n",
       "4   1  24   3  49   1   3   3   4   4  53 ...   1   0   1   0   0   0   0   0   \n",
       "\n",
       "   23  24  \n",
       "0   1   1  \n",
       "1   1   2  \n",
       "2   0   1  \n",
       "3   1   1  \n",
       "4   1   2  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/german.data-numeric'\n",
    "#col_names = ['Account Status', 'Duration', 'Credit History', 'Purpose', 'Credit Amount', 'Savings account/bonds', 'Present Employment Since', 'Installment Rate as % of Income', 'Personal status and sex', 'Other debtors / guarantors', 'Present residence since', 'Property', 'Age ','Other installment plans','Housing','Number of existing credits at this bank','Job','Number of people being liable to provide maintenance for','Telephone','foreign worker','class']\n",
    "german_credit_df = pd.read_table(url, header=None, sep='\\\\s+')\n",
    "german_credit_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 24)\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "X = german_credit_df.loc[:, :23]\n",
    "y = german_credit_df.loc[:, 24]\n",
    "print(X.shape)\n",
    "print(len(y.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision tree train/test accuracies 1.000/0.694\n",
      "Bagging train/test accuracies 1.000/0.773\n",
      "AdaBoost train/test accuracies 1.000/0.685\n",
      "RandomForest train/test accuracies 1.000/0.759\n"
     ]
    }
   ],
   "source": [
    "# do stratified K Fold splitting and Benchmark the data\n",
    "split_data_and_benchmark_clf(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**References**  \n",
    "[1] Python Machine Learning - Sebastian Raschka  \n",
    "[2] Introduction to Data Mining - Pang Ning Tan, Michael Steinbach, Vipin Kumar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
